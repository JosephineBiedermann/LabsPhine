Feedback Flo on LAB ML regression

01, 02, 03: great job. Keep in mind when using .distplot() from seaborn: It will be deprecated soon. I would recommend start using .displot()
when you check for multicolinearity, and plot it in a heatmap, you want to chose a diverging colormap, so that "the 0 sticks out", since Pearson's r ranges from -1 to 1. Check the solution notebook for this. you can use .heatmap(..., center=0, ...) to center the colormap at 0. In your case - i think you chose the default colormap "Inferno", which would be more fitting if your values started at 0 and would go up (or down) to potentially infinity. But reasonable conclusion here, regarding dropping.
04 preprocessing: you're doing the filtering for outliers according to the IQR method correctly, but you store that data into your data dataframe, which you don't continue using anymore in the following process. Here you - intentionally/accidentally? :slightly_smiling_face: - did the right thing, namely not filtering for outliers at all, because there are not enough to worsen our model.
the column effective_to_date is of course best to be converted to datetime *before* preparing it for ML, good job here! Because that way you can calculate with it most comfortably, especially when it comes to feature engineering. But when it comes to *preprocessing*, means, preparing it for ML, we want to have it as an ordinal variable - so that the ML algorithms can calculated with it. You can do that with data['effective_to_date']=data['effective_to_date'].apply(lambda x: x.toordinal())
05 modelling, good job, fairly straight forward
06 model validation: of course, we only learned about evaluating a linear regression model looking at R2 and some of the errors. In a real world scenario, you would look at a lot of other metrics (most comfortable accessible using the statsmodels library, not sklearn :wink: - more on that later in the course). Bur your R2 is roughly in the range that I get in the sample solution, good job.
for visualisation of the model quality, you can always create a scatterplot between y_test and y_predicted - for a perfect model, the dots should be on a line. And you can also plot a histogram of the residuals y_predicted - y_test. If the data are best suited for a linear model, that histogram should show a normal distribution - check the solutions for this.
regarding your MAE: Of course, thats quite a "high" number, but keep in mind, thats the average error in units of the target variable - the total claim amount. And the mean of the total claim amount is around $430, with a standard deviation of around $290 (so the values scatter a lot around that mean, and an MAE that high is not unexpected). The MAE is not the best metric to determine your model quality in the first place, but if you do, always assess with the order of magnitude of the values of your target in the back of your head
regarding your MSE: That one is the MAE, but squared, that means it "punishes" outliers. A datapoint that is 2*d further away from the model linecontributes 4 times more to the MSE than one datapoint that is only d away. Practical! Because that way we get a sense of how many outliers we have, but inconvenient, because now we can't interpret our error anymore in units of the target variable. And a high MSE can just mean: we have a couple of outliers (which we have), but it doesn't have to mean that our model is bad.
regarding your RMSE: that one is probably the most popular metric, since it has all the advantages (punishing outliers, is in unit of the target) and none of the disadvantages of the previous metrics. Your RMSE of $145 is small compared to the average values of the target variable (so no worries here) and it is a bit bigger than the MAE - which tells us, there are some outliers at work here that let the RMSE increase a bit. The outliers can be seen if you plot y_predicted vs y_test.
All in all, good evaluation, great job, Phine! Indeed, the linear regression as a model is justified. And of course you could do some more preprocessing, try to engineer some more features even. This could lead to a better model (meaning better R²). But an R² of ca. 0.7 is already pretty good - and especially if these were real world data. You could do some more interpretation work here, using the coefficients (check the solution for that), because with these you can express your model results very vividly ("According to my data, an increase in $1000 in income leads to a decrease of $X of total claim amount"). Also: Later in the course we would btw. assess also the statistical significance of the coefficients, meaning "can we really say that this feature has a statistical significant impact on the target, or is it just due to pure randomness?"
